{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c9bd1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dmitry/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchcrf import CRF\n",
    "from deeppavlov.models.embedders.fasttext_embedder import FasttextEmbedder\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize.util import align_tokens\n",
    "from glob import glob\n",
    "from functools import partial\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from conlleval import evaluate as prec_rec_f\n",
    "from brat_format import read_file, BratDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b704155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def span_sentences(text, shift=0):\n",
    "    \"\"\"\n",
    "    Extracts sentences and their spans from text.\n",
    "\n",
    "    Parameters\n",
    "    text : str\n",
    "        Text to extract sentences and spans from.\n",
    "    shift : int\n",
    "        Initial position from which to start counting span.\n",
    "\n",
    "    Returns\n",
    "    sents : List[str]\n",
    "        Sentences extracted from text.\n",
    "    spans : List[Tuple[int, int]]\n",
    "        Extracted sentences position in text.\n",
    "    \"\"\"\n",
    "    \n",
    "    sents = sent_tokenize(text, language=\"russian\")\n",
    "    spans = align_tokens(sents, text)\n",
    "    spans = [(start + shift, end + shift) for start, end in spans]\n",
    "    \n",
    "    return sents, spans\n",
    "\n",
    "\n",
    "def span_tokens(text, shift=0):\n",
    "    \"\"\"\n",
    "    Extracts tokens and their spans from text.\n",
    "\n",
    "    Parameters\n",
    "    text : str\n",
    "        Text to extract tokens and spans from.\n",
    "    shift : int\n",
    "        Initial position from which to start counting span.\n",
    "\n",
    "    Returns\n",
    "    tokens : List[str]\n",
    "        Tokens extracted from text.\n",
    "    spans : List[Tuple[int, int]]\n",
    "        Extracted tokens position in text.\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens, spans = [], []\n",
    "\n",
    "    for tok in re.finditer(r\"([^\\W_]+|\\S)\", text):\n",
    "        tokens.append(tok.group(1))\n",
    "        spans.append((shift + tok.start(1), \n",
    "                      shift + tok.end(1)))\n",
    "    \n",
    "    return tokens, spans\n",
    "\n",
    "\n",
    "def to_conll(brat_ners, spans):\n",
    "    \"\"\"\n",
    "    Converts named entities from brat to conll format. In conll format every \n",
    "    token has a tag:\n",
    "    B-named_entity_type - for the first token in named entity,\n",
    "    I-named_entity_type - for a token of named entity that is not first,\n",
    "    O - for a token out of named entity.\n",
    "\n",
    "    Parameters\n",
    "    brat_ners : List[Dict]\n",
    "        Named entities in brat format.\n",
    "    spans : List[Tuple[int, int]]\n",
    "        Position of tokens in reference text.\n",
    "\n",
    "    Returns\n",
    "    conll_ners : List[str]\n",
    "        Conll tags of the tokens corresponding to spans.\n",
    "    \"\"\"\n",
    "    \n",
    "    conll_ners = []\n",
    "\n",
    "    for token_start, token_end in spans:\n",
    "        \n",
    "        for ner in brat_ners:\n",
    "            \n",
    "            if (ner[\"start\"] <= token_start) and (ner[\"end\"] >= token_end):\n",
    "                prefix = \"I\" if (ner[\"start\"] < token_start) else \"B\"\n",
    "                conll_ners.append(prefix + \"-\" + ner[\"ner_type\"])\n",
    "                break\n",
    "        \n",
    "        else:\n",
    "            conll_ners.append(\"O\")  \n",
    "    \n",
    "    return conll_ners\n",
    "\n",
    "\n",
    "def to_brat(conll_ners, spans, ner_id=1):\n",
    "    \"\"\"\n",
    "    Converts named entities from conll to brat format. In brat format every \n",
    "    named entity is represented with its id, type, and position in reference \n",
    "    text.\n",
    "\n",
    "    Parameters\n",
    "    conll_ners : List[str]\n",
    "        Conll tags of the tokens corresponding to spans.\n",
    "    spans : List[tuple[int]]\n",
    "        Position of tokens in reference text.\n",
    "    ner_id : int\n",
    "        The initial id from which to start counting ner_ids\n",
    "\n",
    "    Returns\n",
    "    brat_ners : List[Dict]\n",
    "        Named entities in brat format.\n",
    "    \"\"\"\n",
    "\n",
    "    brat_ners = []\n",
    "    prev = \"O\"\n",
    "\n",
    "    for tag, (token_start, token_end)  in zip(conll_ners, spans):\n",
    "        splitted_tag = tag.split(\"-\")\n",
    "        \n",
    "        if len(splitted_tag) > 1:\n",
    "            prefix, ner_type = splitted_tag\n",
    "            \n",
    "            if prefix == \"I\":\n",
    "                \n",
    "                if prev != \"O\":\n",
    "                    brat_ners[-1][\"end\"] = token_end\n",
    "                    prev = \"I\"\n",
    "                    continue\n",
    "            \n",
    "            brat_ners.append({\"ner_id\": ner_id, \n",
    "                              \"ner_type\": ner_type, \n",
    "                              \"start\": token_start, \n",
    "                              \"end\": token_end})\n",
    "            prev = \"B\"\n",
    "            ner_id += 1\n",
    "        \n",
    "        else:\n",
    "            prev = \"O\"\n",
    "\n",
    "    return brat_ners\n",
    "\n",
    "\n",
    "def extract_data(files):\n",
    "    \"\"\"\n",
    "    Given text sequence as tokens, predicts corresponding conll tags.\n",
    "\n",
    "    Parameters\n",
    "    files : List[str]\n",
    "        Paths to .ann files to extract data from.\n",
    "\n",
    "    Returns\n",
    "    tokens : List[List[str]]\n",
    "        Tokenized text sequences.\n",
    "    tags : List[List[str]]\n",
    "        Conll tags corresponding to token sequences.\n",
    "    \"\"\"\n",
    "\n",
    "    tokens, tags = [], []\n",
    "\n",
    "    for file_path in tqdm(files):\n",
    "        brat_doc = read_file(file_path)\n",
    "        doc_ners = [{\"id\": i, \n",
    "                    \"ner_type\": brat_doc.ners[idx][0], \n",
    "                    \"start\": brat_doc.ners[idx][1], \n",
    "                    \"end\": brat_doc.ners[idx][2]} \n",
    "                    for i, idx in brat_doc.ner_id_2_idx.items()]\n",
    "        \n",
    "        for line in re.finditer(r\"[^\\n]+(\\n+|$)\", brat_doc.txt_data):\n",
    "            sents, sent_spans = span_sentences(line.group(0), shift=line.start())\n",
    "            \n",
    "            for sent, (sent_start, _) in zip(sents, sent_spans):\n",
    "                toks, spans = span_tokens(sent, shift=sent_start)\n",
    "                tokens.append(toks)\n",
    "                tags.append(to_conll(doc_ners, spans))\n",
    "\n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9419a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1377953/2176244872.py:153: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for file_path in tqdm(files):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd16779e2b8140e69e69c93bfd4b51f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "files = glob(\"data/train/*.ann\")\n",
    "tokens, tags = extract_data(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9084550d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1377953/2176244872.py:153: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for file_path in tqdm(files):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87edde6d75214ffdb510d261f7868804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "files = glob(\"data/train/*.ann\")\n",
    "tokens, tags = extract_data(files)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6905bd5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19026, 2115)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens, val_tokens, train_tags, val_tags = train_test_split(tokens, tags, \n",
    "                                                                  test_size=0.1)\n",
    "len(train_tokens), len(val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8bb1f1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class NER_Dataset(Dataset):\n",
    "    def __init__(self, tag2id, seqs, seq_tags):\n",
    "        self.tag2id = tag2id\n",
    "        self.seqs = [[token.lower() for token in seq] for seq in seqs]\n",
    "        self.seq_tags = [[self.tag2id[tag] for tag in tags] for tags in seq_tags]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.seqs[idx], self.seq_tags[idx]\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db4f6a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I-ACT',\n",
       " 'I-SOC',\n",
       " 'B-SOC',\n",
       " 'B-ACT',\n",
       " 'B-BIN',\n",
       " 'B-MET',\n",
       " 'I-INST',\n",
       " 'I-QUA',\n",
       " 'I-CMP',\n",
       " 'I-ECO',\n",
       " 'B-QUA',\n",
       " 'O',\n",
       " 'I-MET',\n",
       " 'I-BIN',\n",
       " 'B-CMP',\n",
       " 'B-ECO',\n",
       " 'B-INST']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Conll tags encoding\n",
    "tags = list({tag for sent in train_tags for tag in sent})\n",
    "tag2id = {tag: i for i, tag in enumerate(tags)}\n",
    "id2tag = {i: tag for i, tag in enumerate(tags)}\n",
    "\n",
    "with open(\"tags.json\", \"w\") as f:\n",
    "    json.dump(tags, f)\n",
    "\n",
    "tags\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "67683abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_ds = NER_Dataset(tag2id, train_tokens, train_tags)\n",
    "val_ds = NER_Dataset(tag2id, val_tokens, val_tags)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "64ba1d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['органами',\n",
       "   'исполнительной',\n",
       "   'власти',\n",
       "   'алтайского',\n",
       "   'края',\n",
       "   'совместно',\n",
       "   'с',\n",
       "   'алтайкрайстатом',\n",
       "   'будут',\n",
       "   'ежегодно',\n",
       "   'определяться',\n",
       "   'тематики',\n",
       "   'аналитических',\n",
       "   'материалов',\n",
       "   'по',\n",
       "   'актуальным',\n",
       "   'направлениям',\n",
       "   'социально',\n",
       "   '-',\n",
       "   'экономического',\n",
       "   'развития',\n",
       "   'алтайского',\n",
       "   'края',\n",
       "   '.'],\n",
       "  ['3', ')']],\n",
       " [[16,\n",
       "   6,\n",
       "   6,\n",
       "   11,\n",
       "   11,\n",
       "   11,\n",
       "   11,\n",
       "   11,\n",
       "   11,\n",
       "   11,\n",
       "   11,\n",
       "   11,\n",
       "   11,\n",
       "   11,\n",
       "   11,\n",
       "   10,\n",
       "   11,\n",
       "   15,\n",
       "   9,\n",
       "   9,\n",
       "   14,\n",
       "   11,\n",
       "   11,\n",
       "   11],\n",
       "  [11, 11]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "val_ds[:2]\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45869bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, feature_dim, num_classes, \n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.feature_dim = feature_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, 2, bidirectional=True, \n",
    "                            batch_first=True)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc_0 = nn.Linear(2 * hidden_size, feature_dim)\n",
    "        self.Q = nn.Linear(feature_dim, feature_dim)\n",
    "        self.K = nn.Linear(feature_dim, feature_dim)\n",
    "        self.V = nn.Linear(feature_dim, feature_dim)\n",
    "        self.layer_norm = nn.LayerNorm(feature_dim)\n",
    "        self.fc_1 = nn.Linear(feature_dim, num_classes)\n",
    "        self.crf = CRF(num_classes, batch_first=True)\n",
    "\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # LSTM\n",
    "        x_packed = pack_padded_sequence(x, lengths, batch_first=True)\n",
    "        seq_out_packed, _ = self.lstm(x_packed)\n",
    "        seq_out, _ = pad_packed_sequence(seq_out_packed, batch_first=True)\n",
    "        seq_out = self.drop(seq_out)\n",
    "        seq_out = self.fc_0(F.relu(seq_out))\n",
    "\n",
    "        # Attention\n",
    "        Q, K, V = self.Q(seq_out), self.K(seq_out), self.V(seq_out)\n",
    "        attn = torch.bmm(Q, K.transpose(1, 2))\n",
    "        attn /= torch.sqrt(torch.tensor(self.feature_dim, dtype=torch.float))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        out = torch.bmm(attn, V)\n",
    "        out = self.layer_norm(out)\n",
    "\n",
    "        scores = self.fc_1(out)\n",
    "\n",
    "        return scores\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70588053",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ELMoEmbedder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m elmo_embedder \u001b[38;5;241m=\u001b[39m \u001b[43mELMoEmbedder\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      3\u001b[0m                              elmo_output_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melmo\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ELMoEmbedder' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "elmo_embedder = FasttextEmbedder(\"http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz\", \n",
    "                             elmo_output_names=[\"elmo\"])\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c34805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
