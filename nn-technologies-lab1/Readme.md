# Лабораторная работа 1	
## Цель работы:
Научиться реализовывать один из алгоритмов глубокого обучения с нуля.
## Задан
ие:
- Скачать датасет CarDatasets;	
- Реализовать сверточную нейронную сеть с использованием NumPy;
- Реализовать нейронную сеть с оптимизатором согласно варианту задания № 2 (ResNet50 + AdaMax). Архитектуру, указанную в варианте, необходимо реализовать с использованием Torch/Tensorflow/Jax;
- Оценить качество модели на тесте и сравните быстродействие реализованных вариантов;
- Запустить обучение на классическом Adam и сравнить сходимость результатов с вариантом задания;
- Сделать отчёт в виде readme на GitHub.

## Краткие теоретические сведения:
### Свертка:
Основным компонентом сверточной нейронной сети является свертка или как ее еще называют ядро (kernel).
В отличие от линейных слоев она позволяет эффективно обрабатывать пространственную информацию и работать с входным 
сигналом, представленным в виде матрицы, не разрушая пространственные связи. 
Визуализация сверточного слоя и его основные параметры свертки представлены на рисунке 1.
> Рисунок 1 - Сверточный слой
![img_2.png](images/img_2.png)
### ResNet:
До определенного момента сверточные нейронные сети сталкивались с проблемой затухания градиаента при попытке масштабирования 
архитектур. Решением этой проблемы стал "трюк" skip connection. А первой популярной архитектурой, использующей его стала 
ResNet с Residual Connection блоками. 
>Рисунок 2 - Residual Connection
![img_1.png](images/img_1.png)

> Рисунок 3 - Архитектура Resnet
![img_3.png](images/img_3.png)
### Adam & AdaMax: 
Adam — adaptive moment estimation, оптимизационный алгоритм. Он сочетает в себе и идею накопления момента и идею 
более слабого обновления весов для типичных признаков. 
Для накопления момента используются значения градиента
>![img_4.png](images/img_4.png)

, а для оценки "нетипичности" признака используется средняя нецентрированная 
дисперсия.
> ![img_5.png](images/img_5.png)

Начальная калибровка:
>![img_6.png](images/img_6.png)

Итоговое правило обновления:
>![img_7.png](images/img_7.png)

AdaMax отличается от Adam тем, что вместо дисперсии в качестве параметра отвечающего за "нетипичность"
рассчитывается инерционный момент распределения градиентов произвольной степени:
>![img_8.png](images/img_8.png)

## Описание разработанной системы:
### CNN Numpy реализация:
В ходе выполнения работы с помощью библиотеки NumPy были реализованы следующие слои нейронной сети:
- Линейный слой;
- Сверточный слой;
- Функции активации Sigmoid и Softmax;
- Функция потерь Cross Entropy Loss;
- Reshape для преобразования карт признаков в одномерный вектор и подачи в линейный слой.

Код задания расположен в директории cnn_numpy_implementation/
Код проекта был протестирован на датасете MNIST.

>Значения функции потерь для 100 эпох обучения:
>![tran_loss.png](cnn_numpy_implementation%2Ftran_loss.png)

### ResNet PyTorch реализация:
В ходе выполнения работы была реализована архитектура ResNet50 с использованием фреймворка для глубокого обучения PyTorch.
Для обучения сети использовался набор данных "Stanford Cars Dataset".
Графики функции потерь и целевой метрики для оптимизатора Adam:
![resnet_50_losses_adam_optimizer.jpg](resnet50_torch_implementation%2Fplots%2Fresnet_50_losses_adam_optimizer.jpg)
![resnet_50_f1_adam_optimizer.jpg](resnet50_torch_implementation%2Fplots%2Fresnet_50_f1_adam_optimizer.jpg)
Графики функции потерь и целевой функции для оптимизатора AdaMax:
![resnet_50_losses_adamax_optimizer.jpg](resnet50_torch_implementation%2Fplots%2Fresnet_50_losses_adamax_optimizer.jpg)
![resnet_50_f1_adamax_optimizer.jpg](resnet50_torch_implementation%2Fplots%2Fresnet_50_f1_adamax_optimizer.jpg)

## Выводы:
В ходе выполнения работы были получены навыки самостоятельной реализации слоев нейронных сетей, реализации
существующих архитектур с использованием фреймворков для глубокого обучения. Из полученных результатов обучения на двух оптимизаторах
сложно судить о том, какой из них показал себя лучше для заданной архитектуры и набора данных.