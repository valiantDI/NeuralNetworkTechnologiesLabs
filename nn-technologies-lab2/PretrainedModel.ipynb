{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4cab272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from datasets import Dataset, DatasetDict, load_dataset, load_metric\n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ef74106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 100, 1: 100, 2: 96, 3: 89})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(385, 385)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'data/'\n",
    "categories = os.listdir(data_path)\n",
    "paths_and_cats = defaultdict(list)\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    \n",
    "    audio_folder = f'{data_path}{category}/'\n",
    "    audio_files = os.listdir(audio_folder)\n",
    "\n",
    "    for audio_file in audio_files:\n",
    "    \n",
    "        file_path = f'{data_path}{category}/{audio_file}'\n",
    "    \n",
    "        if os.path.getsize(file_path) / (1024 * 1024) < 100:\n",
    "        \n",
    "            paths_and_cats['paths'].append(file_path)\n",
    "            paths_and_cats['labels'].append(i)\n",
    "\n",
    "print(Counter(paths_and_cats['labels']))\n",
    "len(paths_and_cats['paths']), len(paths_and_cats['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f44fc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [06:45<00:00,  1.05s/it]\n"
     ]
    }
   ],
   "source": [
    "arraies = []\n",
    "labels = []\n",
    "sampling_rates = []\n",
    "duration = 60\n",
    "\n",
    "for i in tqdm(range(len(paths_and_cats['labels']))):\n",
    "    data, sr = torchaudio.load(paths_and_cats['paths'][i], format='mp3')\n",
    "    data = torchaudio.functional.resample(data, orig_freq=sr, new_freq=16000)[:, :sr * duration]\n",
    "    if len(data[0]) == 2:\n",
    "        arraies += [np.array((data[0] + data[1])/2)]\n",
    "    else:\n",
    "        arraies += [np.array(data[0])]\n",
    "    sampling_rates += [sr]\n",
    "    labels +=  [paths_and_cats['labels'][i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4fd9024",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_arraies = []\n",
    "df_valid_arraies = []\n",
    "df_train_labels = []\n",
    "df_valid_labels = []\n",
    "limit = 20\n",
    "for i in range(len(labels)):\n",
    "    if df_valid_labels.count(labels[i]) < limit:\n",
    "        df_valid_labels += [labels[i]]\n",
    "        df_valid_arraies +=[arraies[i]]\n",
    "    else:\n",
    "        df_train_labels += [labels[i]]\n",
    "        df_train_arraies += [arraies[i]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c44a8da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(305, 80, (2646000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train_arraies), len(df_valid_arraies), df_train_arraies[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8db42acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = pd.DataFrame({\"array\": df_train_arraies, \"sampling_rate\": [0 for i in df_train_arraies ] , \"label\": df_train_labels })\n",
    "vdf = pd.DataFrame({\"array\": df_valid_arraies, \"sampling_rate\": [0 for i in df_valid_arraies ], \"label\": df_valid_labels })\n",
    "tdf = tdf.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "tds = Dataset.from_pandas(tdf)\n",
    "vds = Dataset.from_pandas(vdf)\n",
    "\n",
    "ds = DatasetDict()\n",
    "ds[\"train\"] =  tds\n",
    "ds[\"validation\"] =  vds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98372425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ASTFeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"feature_extractor_type\": \"ASTFeatureExtractor\",\n",
       "  \"feature_size\": 1,\n",
       "  \"max_length\": 1024,\n",
       "  \"mean\": -4.2677393,\n",
       "  \"num_mel_bins\": 128,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0.0,\n",
       "  \"return_attention_mask\": false,\n",
       "  \"sampling_rate\": 16000,\n",
       "  \"std\": 4.5689974\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53ed643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    audio_arrays = examples[\"array\"]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays, \n",
    "        sampling_rate=feature_extractor.sampling_rate, \n",
    "        max_length=int(feature_extractor.sampling_rate * duration), \n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a6ecb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a2a84fe321143eb85f74170220eeca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/305 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_dataset = ds.map(preprocess_function, remove_columns=[\"sampling_rate\", \"array\"], batched=True, batch_size=1)\n",
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1e8428",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 4\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    \"MIT/ast-finetuned-audioset-10-10-0.4593\", num_labels = 4, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876b1293",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"MIT/ast-finetuned-audioset-10-10-0.4593\".split(\"/\")[1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_total_limit = 3,\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=30,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954f5035",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"f1\", average='macro')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    print(metric.compute(predictions=predictions, references=eval_pred.label_ids, average='macro'))\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59184dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model,\n",
    "                  args,\n",
    "                  train_dataset=encoded_dataset[\"train\"],\n",
    "                  eval_dataset=encoded_dataset[\"validation\"],\n",
    "                  tokenizer=feature_extractor,\n",
    "                  compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0784eb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
